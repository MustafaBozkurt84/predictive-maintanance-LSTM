{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_univariate_timeseries_anomaly.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMH38c982TvPcFDcrGXwaV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustafaBozkurt84/predictive-maintanance-LSTM/blob/master/lstm_univariate_timeseries_anomaly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SsTQcBDtOQJ"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "import pickle\n",
        "register_matplotlib_converters()\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjpOmT3tkjI",
        "outputId": "6eea73e5-cc8a-409c-9427-cef520f514c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzv0QHWVQfwS"
      },
      "source": [
        "df =pd.read_csv(\"/content/drive/MyDrive/Datasets/predictive maintance /31_hidrolikmotoru_analiz.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybgz2IFAt0ZW"
      },
      "source": [
        "df.drop([\"name\",\"partno\",\"balancerbasinci\",\"spm\",\"xkurt\"],axis=1,inplace=True)\n",
        "df.columns=['Time', 'vibx', 'vibz', 'temp', 'zacc', 'zfreq', 'crest']\n",
        "df[\"Time\"]= [str(i).replace(\"2020-02-02\",\"2/2/2020\").replace(\"2020-01-02\",\"2/1/2020\") for i in df[\"Time\"]]\n",
        "\n",
        "df['Time'] = pd.to_datetime(df['Time']) \n",
        "df['Time']=[str(i).split(\":\")[0] for i in df[\"Time\"]]\n",
        "df['Time'] = pd.to_datetime(df['Time'],format=\"%Y-%m-%d %H\") #%Y-%m-%d %H:%M:%S\n",
        "\n",
        "df=df.groupby(\"Time\").mean()\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Datasets/predictive maintance /DataAnaliz.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko97fxi4U5K6"
      },
      "source": [
        "df['Time'] = pd.to_datetime(df['Time'],format=\"%Y-%m-%d %H\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0gcTv7yYYw"
      },
      "source": [
        "# Train Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHocX9PEdIFF"
      },
      "source": [
        "data1=df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT-53qYUaSSL"
      },
      "source": [
        "ariza_tarihleri=[\"2020-01-27 14:00:00\",\"2020-09-11 14:00:00\",\"2020-10-06 18:00:00\",\"2020-10-10 08:00:00\",\"2020-10-13 04:00:00\",\"2020-10-18 00:00:00\",\"2020-10-30 09:00:00\",\"2020-11-02 05:00:00\"]\n",
        "ariza_index = []\n",
        "for i in ariza_tarihleri:\n",
        "  ariza_index.append(df[df[\"Time\"]==i].index[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbIgsRFOaWGL"
      },
      "source": [
        "drop_index_list=[]\n",
        "for a in ariza_index:\n",
        "\n",
        "  for i in range(0,24*7):\n",
        "    drop_index_list.append(a+i)\n",
        "  for i in range(0,24*2):\n",
        "    drop_index_list.append(a-i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlA2k7EPaZSb"
      },
      "source": [
        "df.drop(index=drop_index_list,axis=0,inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyMjvWpUbNMh"
      },
      "source": [
        "data =df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IgF75CLn61w"
      },
      "source": [
        "ariza_tarihleri=[\"2020-01-27 14:00:00\",\"2020-09-11 14:00:00\",\"2020-10-06 18:00:00\",\"2020-10-10 08:00:00\",\"2020-10-13 04:00:00\",\"2020-10-18 00:00:00\",\"2020-10-30 09:00:00\",\"2020-11-02 05:00:00\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxQthLbPewfx"
      },
      "source": [
        "def pickle_all(key,value):\n",
        "         pickle_out = open(key, \"wb\")\n",
        "         pickle.dump(value, pickle_out)\n",
        "         pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps461nmozA_f"
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)        \n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfsEi1MmtK1"
      },
      "source": [
        "def lstm_anomaly(df,col,time_step,threshold):\n",
        "    print(f\"{col}\"+50*\"*\")\n",
        "    df  = data.copy()\n",
        "    df.set_index(\"Time\",inplace=True)\n",
        "    df1=df[[col]]\n",
        "    df2=data1.copy()\n",
        "    print(df2)\n",
        "    df2.set_index(\"Time\",inplace= True)\n",
        "    print(df2)\n",
        "    df3=df2[[col]]\n",
        "    train_size=df2.shape[0]\n",
        "    train, test = df1.iloc[24*30:train_size], df3[df3.index>\"2020-01-01 15:00:00\"]\n",
        "    print(train.shape, test.shape)\n",
        "    # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
        "    \n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(train[[col]])\n",
        "\n",
        "    train[col] = scaler.transform(train[[col]])\n",
        "    test[col] = scaler.transform(test[[col]])\n",
        "    pickle_all(f\"/content/drive/MyDrive/model_predictive_maintanence/{col}_scaler_lstm_univarate.pkl\",scaler)    \n",
        "           \n",
        "    TIME_STEPS = time_step\n",
        "    \n",
        "    # reshape to [samples, time_steps, n_features]\n",
        "\n",
        "    X_train, y_train = create_dataset(train[[col]], train[col], TIME_STEPS)\n",
        "    X_test, y_test = create_dataset(test[[col]], test[col], TIME_STEPS)\n",
        "\n",
        "    print(X_train.shape)\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LSTM(\n",
        "            units=64, \n",
        "            input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "        ))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
        "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "        \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        shuffle=False,verbose=1\n",
        "    )\n",
        "   \n",
        "    model.save(f\"/content/drive/MyDrive/model_predictive_maintanence/{col}_anomaly_model.h5\")\n",
        "    X_train_pred = model.predict(X_train)\n",
        "\n",
        "    train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
        "    \n",
        "    X_test_pred = model.predict(X_test)\n",
        "\n",
        "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
        "    \n",
        "    THRESHOLD = threshold\n",
        "\n",
        "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
        "    test_score_df['loss'] = test_mae_loss\n",
        "    test_score_df['threshold'] = THRESHOLD\n",
        "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
        "    test_score_df[col] = test[TIME_STEPS:][col]\n",
        "    \n",
        "    anomalies = test_score_df[test_score_df.anomaly == True]\n",
        "    \n",
        "    train_score_df = pd.DataFrame(index=train[TIME_STEPS:].index)\n",
        "    train_score_df['loss'] = train_mae_loss\n",
        "    train_score_df['threshold'] = THRESHOLD\n",
        "    train_score_df['anomaly'] = train_score_df.loss > train_score_df.threshold\n",
        "    train_score_df[col] = train[TIME_STEPS:][col]\n",
        "    \n",
        "    anomalies_train = train_score_df[train_score_df.anomaly == True]\n",
        "    \n",
        "    return anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BaQvOUlXJU"
      },
      "source": [
        "def loss_and_threshold_value_plot():\n",
        "    plt.plot(test_score_df.index, test_score_df.loss, label='loss_'+col)\n",
        "    plt.plot(test_score_df.index, test_score_df.threshold, label='threshold_'+col)\n",
        "    plt.xticks(rotation=25)\n",
        "    plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketDomXKkur1"
      },
      "source": [
        "def train_mae_loss():\n",
        "    sns.distplot(train_mae_loss, bins=50, kde=True);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btNF9-XSj9G1"
      },
      "source": [
        "def anomaly_plot(test,anomalies,col):\n",
        "    ariza_tarihleri_plot= []\n",
        "    for i in [str(a) for a in data1[\"Time\"]]:\n",
        "          if i not in [str(b) for b in ariza_tarihleri]:\n",
        "            ariza_tarihleri_plot.append(data1[col].min())\n",
        "          else:\n",
        "            ariza_tarihleri_plot.append(data1[col].max())\n",
        "    ariza=pd.DataFrame(ariza_tarihleri_plot)\n",
        "    ariza.index=data1[\"Time\"]\n",
        "    ariza.columns=[\"ariza tarihleri\"]\n",
        "\n",
        "    print(anomalies.head(10))\n",
        "    plt.figure(figsize=(24, 12), dpi=80)\n",
        "    plt.plot(\n",
        "          test[TIME_STEPS:].index, \n",
        "          scaler.inverse_transform(test[TIME_STEPS:][col]), \n",
        "          label=col\n",
        "        );\n",
        "\n",
        "    sns.scatterplot(\n",
        "          anomalies.index,\n",
        "          scaler.inverse_transform(anomalies[col]),\n",
        "          color=sns.color_palette()[3],\n",
        "          s=52,\n",
        "          label='anomaly_'+col\n",
        "        )\n",
        "    plt.plot(ariza[\"ariza tarihleri\"],label=\"ariza tarihleri\")\n",
        "    plt.xticks(rotation=25)\n",
        "    plt.legend();\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76WgbsIKkfYO"
      },
      "source": [
        "def loss_plot():\n",
        "    plt.plot(history.history['loss'], label='train loss_'+col)\n",
        "    plt.plot(history.history['val_loss'], label='test loss_'+col)\n",
        "    plt.legend();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI2rb1iz2cpp"
      },
      "source": [
        "total_anomalies = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIt27X8NB1Fa"
      },
      "source": [
        "# vibx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfo7W8W7o-aj"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"vibx\", time_step=24, threshold=1.4)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszCwJkypLqw"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBtLUXSnYX2"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akmpjNKBnhbL"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"vibx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-XjgNoqB7FE"
      },
      "source": [
        "# vibz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k88E46sexEy0"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"vibz\",time_step=24,threshold=1.7)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03SkwRnUxH3L"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTatZXISxpSH"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTJzzwexuE7"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"vibz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rCG4S5GCBY5"
      },
      "source": [
        "# temp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX1DNvOPyXXi"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"temp\",time_step=24,threshold=1.2)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jpn6R5Uygfq"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn4Jo_Bwyh4q"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EXCYNfAyjQS"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"temp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fNx4wK6CGpJ"
      },
      "source": [
        "# zacc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9dnYE7xymgQ"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"zacc\",time_step=24,threshold=1)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ixyCZsSys3_"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyrOLEyys2W"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45Bse-Cysy_"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"zacc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpT8vogpCMfB"
      },
      "source": [
        "# crest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aovrcDUhysre"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"crest\",time_step=24,threshold=2.4)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfdSSLH3yspE"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7-KR4ajysid"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0xzqyFiysXc"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"crest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfOg66ciCSSa"
      },
      "source": [
        "# zfreq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J24XlX6jzBvM"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"zfreq\",time_step=24,threshold=2)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6frieDYzBss"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbm68dngzBqH"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnwIM-MvzBnq"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"zfreq\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOjMY7TB7K1"
      },
      "source": [
        "ariza_tarihleri_plot= []\n",
        "for i in [str(a) for a in data[\"Time\"]]:\n",
        "          if i not in [str(b) for b in ariza_tarihleri]:\n",
        "            ariza_tarihleri_plot.append(1)\n",
        "          else:\n",
        "            ariza_tarihleri_plot.append(5)\n",
        "ariza=pd.DataFrame(ariza_tarihleri_plot)\n",
        "ariza.index=data[\"Time\"]\n",
        "ariza.columns=[\"ariza tarihleri\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XV99BiY4qLG"
      },
      "source": [
        "total_anomalies.groupby(\"Time\")[\"anomaly_count\"].sum().plot()\n",
        "ariza[\"ariza tarihleri\"].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHk5vpqx4ysL"
      },
      "source": [
        "total_anomalies[\"Time\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7wnlDRTFAXB"
      },
      "source": [
        "total_anomalies.to_csv(\"total_anomaly.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44OcB13wdYa8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}