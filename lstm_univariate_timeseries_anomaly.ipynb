{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_univariate_timeseries_anomaly.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeVrH45BE5G5okDTo0UJtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustafaBozkurt84/predictive-maintanance-LSTM/blob/master/lstm_univariate_timeseries_anomaly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SsTQcBDtOQJ"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "import pickle\n",
        "register_matplotlib_converters()\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjpOmT3tkjI",
        "outputId": "f6c738b2-559e-4daf-aba5-83af2f988370"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzv0QHWVQfwS",
        "outputId": "aa836a43-7a89-4b81-a975-4287d447318a"
      },
      "source": [
        "df =pd.read_csv(\"/content/drive/MyDrive/Datasets/predictive maintance /31_hidrolikmotoru_analiz.csv\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Ybgz2IFAt0ZW",
        "outputId": "7618d1a5-537c-4ba0-a915-561c66ca5314"
      },
      "source": [
        "df.drop([\"name\",\"partno\",\"balancerbasinci\",\"spm\",\"xkurt\"],axis=1,inplace=True)\n",
        "df.columns=['Time', 'vibx', 'vibz', 'temp', 'zacc', 'zfreq', 'crest']\n",
        "df[\"Time\"]= [str(i).replace(\"2020-02-02\",\"2/2/2020\").replace(\"2020-01-02\",\"2/1/2020\") for i in df[\"Time\"]]\n",
        "\n",
        "df['Time'] = pd.to_datetime(df['Time']) \n",
        "df['Time']=[str(i).split(\":\")[0] for i in df[\"Time\"]]\n",
        "df['Time'] = pd.to_datetime(df['Time'],format=\"%Y-%m-%d %H\") #%Y-%m-%d %H:%M:%S\n",
        "\n",
        "df=df.groupby(\"Time\").mean()\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Datasets/predictive maintance /DataAnaliz.csv\")\n",
        "df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>vibx</th>\n",
              "      <th>vibz</th>\n",
              "      <th>temp</th>\n",
              "      <th>zacc</th>\n",
              "      <th>zfreq</th>\n",
              "      <th>crest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-01 00:00:00</td>\n",
              "      <td>0.154338</td>\n",
              "      <td>0.144321</td>\n",
              "      <td>15.637208</td>\n",
              "      <td>0.041157</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.111596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-01 01:00:00</td>\n",
              "      <td>0.156064</td>\n",
              "      <td>0.145520</td>\n",
              "      <td>15.373346</td>\n",
              "      <td>0.041440</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.160439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-01 02:00:00</td>\n",
              "      <td>0.155957</td>\n",
              "      <td>0.146585</td>\n",
              "      <td>15.122024</td>\n",
              "      <td>0.041997</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.212914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-01 03:00:00</td>\n",
              "      <td>0.155334</td>\n",
              "      <td>0.143598</td>\n",
              "      <td>14.902892</td>\n",
              "      <td>0.043065</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.331691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-01 04:00:00</td>\n",
              "      <td>0.157375</td>\n",
              "      <td>0.144605</td>\n",
              "      <td>14.703281</td>\n",
              "      <td>0.047041</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.693571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Time      vibx      vibz       temp      zacc  zfreq     crest\n",
              "0 2020-01-01 00:00:00  0.154338  0.144321  15.637208  0.041157   0.01  4.111596\n",
              "1 2020-01-01 01:00:00  0.156064  0.145520  15.373346  0.041440   0.01  4.160439\n",
              "2 2020-01-01 02:00:00  0.155957  0.146585  15.122024  0.041997   0.01  4.212914\n",
              "3 2020-01-01 03:00:00  0.155334  0.143598  14.902892  0.043065   0.01  4.331691\n",
              "4 2020-01-01 04:00:00  0.157375  0.144605  14.703281  0.047041   0.01  4.693571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko97fxi4U5K6"
      },
      "source": [
        "df['Time'] = pd.to_datetime(df['Time'],format=\"%Y-%m-%d %H\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0gcTv7yYYw"
      },
      "source": [
        "# Train Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHocX9PEdIFF"
      },
      "source": [
        "data1=df"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT-53qYUaSSL"
      },
      "source": [
        "ariza_tarihleri=[\"2020-01-27 14:00:00\",\"2020-09-11 14:00:00\",\"2020-10-06 18:00:00\",\"2020-10-10 08:00:00\",\"2020-10-13 04:00:00\",\"2020-10-18 00:00:00\",\"2020-10-30 09:00:00\",\"2020-11-02 05:00:00\"]\n",
        "ariza_index = []\n",
        "for i in ariza_tarihleri:\n",
        "  ariza_index.append(df[df[\"Time\"]==i].index[0])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbIgsRFOaWGL"
      },
      "source": [
        "drop_index_list=[]\n",
        "for a in ariza_index:\n",
        "\n",
        "  for i in range(0,24*7):\n",
        "    drop_index_list.append(a+i)\n",
        "  for i in range(0,24*2):\n",
        "    drop_index_list.append(a-i)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlA2k7EPaZSb"
      },
      "source": [
        "df.drop(index=drop_index_list,axis=0,inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyMjvWpUbNMh"
      },
      "source": [
        "data =df.copy()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IgF75CLn61w"
      },
      "source": [
        "ariza_tarihleri=[\"2020-01-27 14:00:00\",\"2020-09-11 14:00:00\",\"2020-10-06 18:00:00\",\"2020-10-10 08:00:00\",\"2020-10-13 04:00:00\",\"2020-10-18 00:00:00\",\"2020-10-30 09:00:00\",\"2020-11-02 05:00:00\"]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxQthLbPewfx"
      },
      "source": [
        "def pickle_all(key,value):\n",
        "         pickle_out = open(key, \"wb\")\n",
        "         pickle.dump(value, pickle_out)\n",
        "         pickle_out.close()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps461nmozA_f"
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)        \n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfsEi1MmtK1"
      },
      "source": [
        "def lstm_anomaly(df,col,date,time_step,threshold):\n",
        "    print(f\"{col}\"+50*\"*\")\n",
        "    df  = data.copy()\n",
        "    df.set_index(\"Time\",inplace=True)\n",
        "    df1=df[[col]]\n",
        "    df2=data1.copy()\n",
        "    print(df2)\n",
        "    df2.set_index(\"Time\",inplace= True)\n",
        "    print(df2)\n",
        "    df3=df2[[col]]\n",
        "    train_size=df2.shape[0]\n",
        "    train, test = df1.iloc[24*30:train_size], df3[df3.index>\"2020-01-01 15:00:00\"]\n",
        "    print(train.shape, test.shape)\n",
        "    # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
        "    \n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(train[[col]])\n",
        "\n",
        "    train[col] = scaler.transform(train[[col]])\n",
        "    test[col] = scaler.transform(test[[col]])\n",
        "    pickle_all(f\"/content/drive/MyDrive/model_predictive_maintanence/{col}_scaler_lstm_univarate.pkl\",scaler)    \n",
        "           \n",
        "    TIME_STEPS = time_step\n",
        "    \n",
        "    # reshape to [samples, time_steps, n_features]\n",
        "\n",
        "    X_train, y_train = create_dataset(train[[col]], train[col], TIME_STEPS)\n",
        "    X_test, y_test = create_dataset(test[[col]], test[col], TIME_STEPS)\n",
        "\n",
        "    print(X_train.shape)\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LSTM(\n",
        "            units=64, \n",
        "            input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "        ))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
        "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "        \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        shuffle=False,verbose=1\n",
        "    )\n",
        "   \n",
        "    model.save(f\"/content/drive/MyDrive/model_predictive_maintanence/{col}_anomaly_model.h5\")\n",
        "    X_train_pred = model.predict(X_train)\n",
        "\n",
        "    train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
        "    \n",
        "    X_test_pred = model.predict(X_test)\n",
        "\n",
        "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
        "    \n",
        "    THRESHOLD = threshold\n",
        "\n",
        "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
        "    test_score_df['loss'] = test_mae_loss\n",
        "    test_score_df['threshold'] = THRESHOLD\n",
        "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
        "    test_score_df[col] = test[TIME_STEPS:][col]\n",
        "    \n",
        "    anomalies = test_score_df[test_score_df.anomaly == True]\n",
        "    \n",
        "    train_score_df = pd.DataFrame(index=train[TIME_STEPS:].index)\n",
        "    train_score_df['loss'] = train_mae_loss\n",
        "    train_score_df['threshold'] = THRESHOLD\n",
        "    train_score_df['anomaly'] = train_score_df.loss > train_score_df.threshold\n",
        "    train_score_df[col] = train[TIME_STEPS:][col]\n",
        "    \n",
        "    anomalies_train = train_score_df[train_score_df.anomaly == True]\n",
        "    \n",
        "    return anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BaQvOUlXJU"
      },
      "source": [
        "def loss_and_threshold_value_plot():\n",
        "    plt.plot(test_score_df.index, test_score_df.loss, label='loss_'+col)\n",
        "    plt.plot(test_score_df.index, test_score_df.threshold, label='threshold_'+col)\n",
        "    plt.xticks(rotation=25)\n",
        "    plt.legend();"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketDomXKkur1"
      },
      "source": [
        "def train_mae_loss():\n",
        "    sns.distplot(train_mae_loss, bins=50, kde=True);\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btNF9-XSj9G1"
      },
      "source": [
        "def anomaly_plot(test,anomalies,col):\n",
        "    ariza_tarihleri_plot= []\n",
        "    for i in [str(a) for a in data1[\"Time\"]]:\n",
        "          if i not in [str(b) for b in ariza_tarihleri]:\n",
        "            ariza_tarihleri_plot.append(data1[col].min())\n",
        "          else:\n",
        "            ariza_tarihleri_plot.append(data1[col].max())\n",
        "    ariza=pd.DataFrame(ariza_tarihleri_plot)\n",
        "    ariza.index=data1[\"Time\"]\n",
        "    ariza.columns=[\"ariza tarihleri\"]\n",
        "\n",
        "    print(anomalies.head(10))\n",
        "    plt.figure(figsize=(24, 12), dpi=80)\n",
        "    plt.plot(\n",
        "          test[TIME_STEPS:].index, \n",
        "          scaler.inverse_transform(test[TIME_STEPS:][col]), \n",
        "          label=col\n",
        "        );\n",
        "\n",
        "    sns.scatterplot(\n",
        "          anomalies.index,\n",
        "          scaler.inverse_transform(anomalies[col]),\n",
        "          color=sns.color_palette()[3],\n",
        "          s=52,\n",
        "          label='anomaly_'+col\n",
        "        )\n",
        "    plt.plot(ariza[\"ariza tarihleri\"],label=\"ariza tarihleri\")\n",
        "    plt.xticks(rotation=25)\n",
        "    plt.legend();\n",
        "    plt.show()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76WgbsIKkfYO"
      },
      "source": [
        "def loss_plot():\n",
        "    plt.plot(history.history['loss'], label='train loss_'+col)\n",
        "    plt.plot(history.history['val_loss'], label='test loss_'+col)\n",
        "    plt.legend();\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI2rb1iz2cpp"
      },
      "source": [
        "total_anomalies = pd.DataFrame()\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIt27X8NB1Fa"
      },
      "source": [
        "# vibx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfo7W8W7o-aj",
        "outputId": "93ec5a45-e362-41d1-c152-6c7cd58e2962"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"vibx\",date=\"2021-01-10 04:00:00\", time_step=20, threshold=1.9)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vibx**************************************************\n",
            "                    Time      vibx      vibz  ...      zacc     zfreq     crest\n",
            "0    2020-01-01 00:00:00  0.154338  0.144321  ...  0.041157  0.010000  4.111596\n",
            "1    2020-01-01 01:00:00  0.156064  0.145520  ...  0.041440  0.010000  4.160439\n",
            "2    2020-01-01 02:00:00  0.155957  0.146585  ...  0.041997  0.010000  4.212914\n",
            "3    2020-01-01 03:00:00  0.155334  0.143598  ...  0.043065  0.010000  4.331691\n",
            "4    2020-01-01 04:00:00  0.157375  0.144605  ...  0.047041  0.010000  4.693571\n",
            "...                  ...       ...       ...  ...       ...       ...       ...\n",
            "7123 2021-01-22 02:00:00  1.696243  1.339870  ...  0.669262  0.134723  5.019967\n",
            "7124 2021-01-22 03:00:00  1.638439  1.249788  ...  0.652285  0.130145  5.058305\n",
            "7125 2021-01-22 04:00:00  1.624994  1.199024  ...  0.641953  0.128047  5.066521\n",
            "7126 2021-01-22 05:00:00  1.639610  1.219718  ...  0.643970  0.129403  5.031291\n",
            "7127 2021-01-22 06:00:00  1.616865  1.137539  ...  0.644223  0.129534  5.010440\n",
            "\n",
            "[7128 rows x 7 columns]\n",
            "                         vibx      vibz  ...     zfreq     crest\n",
            "Time                                     ...                    \n",
            "2020-01-01 00:00:00  0.154338  0.144321  ...  0.010000  4.111596\n",
            "2020-01-01 01:00:00  0.156064  0.145520  ...  0.010000  4.160439\n",
            "2020-01-01 02:00:00  0.155957  0.146585  ...  0.010000  4.212914\n",
            "2020-01-01 03:00:00  0.155334  0.143598  ...  0.010000  4.331691\n",
            "2020-01-01 04:00:00  0.157375  0.144605  ...  0.010000  4.693571\n",
            "...                       ...       ...  ...       ...       ...\n",
            "2021-01-22 02:00:00  1.696243  1.339870  ...  0.134723  5.019967\n",
            "2021-01-22 03:00:00  1.638439  1.249788  ...  0.130145  5.058305\n",
            "2021-01-22 04:00:00  1.624994  1.199024  ...  0.128047  5.066521\n",
            "2021-01-22 05:00:00  1.639610  1.219718  ...  0.129403  5.031291\n",
            "2021-01-22 06:00:00  1.616865  1.137539  ...  0.129534  5.010440\n",
            "\n",
            "[7128 rows x 6 columns]\n",
            "(6408, 1) (6850, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(6388, 20, 1)\n",
            "Epoch 1/50\n",
            "180/180 [==============================] - 9s 30ms/step - loss: 0.2716 - val_loss: 0.1684\n",
            "Epoch 2/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.2152 - val_loss: 0.1451\n",
            "Epoch 3/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1956 - val_loss: 0.1399\n",
            "Epoch 4/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1869 - val_loss: 0.1306\n",
            "Epoch 5/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1792 - val_loss: 0.1289\n",
            "Epoch 6/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1756 - val_loss: 0.1237\n",
            "Epoch 7/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1702 - val_loss: 0.1233\n",
            "Epoch 8/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1684 - val_loss: 0.1221\n",
            "Epoch 9/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1683 - val_loss: 0.1266\n",
            "Epoch 10/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1669 - val_loss: 0.1198\n",
            "Epoch 11/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1687 - val_loss: 0.1281\n",
            "Epoch 12/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1682 - val_loss: 0.1174\n",
            "Epoch 13/50\n",
            "180/180 [==============================] - 4s 25ms/step - loss: 0.1681 - val_loss: 0.1257\n",
            "Epoch 14/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1672 - val_loss: 0.1197\n",
            "Epoch 15/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1684 - val_loss: 0.1257\n",
            "Epoch 16/50\n",
            "180/180 [==============================] - 4s 24ms/step - loss: 0.1669 - val_loss: 0.1235\n",
            "Epoch 17/50\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1663"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszCwJkypLqw"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBtLUXSnYX2"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akmpjNKBnhbL"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"vibx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-XjgNoqB7FE"
      },
      "source": [
        "# vibz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k88E46sexEy0"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"vibz\",date=\"2021-01-10 04:00:00\",time_step=20,threshold=1.9)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03SkwRnUxH3L"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTatZXISxpSH"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTJzzwexuE7"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"vibz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rCG4S5GCBY5"
      },
      "source": [
        "# temp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX1DNvOPyXXi"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"temp\",date=\"2021-01-10 04:00:00\",time_step=20,threshold=1.5)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jpn6R5Uygfq"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn4Jo_Bwyh4q"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EXCYNfAyjQS"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"temp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fNx4wK6CGpJ"
      },
      "source": [
        "# zacc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9dnYE7xymgQ"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"zacc\",date=\"2021-01-10 04:00:00\",time_step=20,threshold=2.5)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ixyCZsSys3_"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyrOLEyys2W"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45Bse-Cysy_"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"zacc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpT8vogpCMfB"
      },
      "source": [
        "# crest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aovrcDUhysre"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"crest\",date=\"2021-01-10 04:00:00\",time_step=20,threshold=4)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfdSSLH3yspE"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7-KR4ajysid"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0xzqyFiysXc"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"crest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfOg66ciCSSa"
      },
      "source": [
        "# zfreq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J24XlX6jzBvM"
      },
      "source": [
        "anomalies,anomalies_train,test,train,test_score_df,test_mae_loss,train_mae_loss,history,TIME_STEPS,col,scaler= lstm_anomaly(df,col=\"zfreq\",date=\"2021-01-10 04:00:00\",time_step=20,threshold=3.2)\n",
        "anomalyy= anomalies.reset_index()\n",
        "anomalyy[\"anomaly_count\"]=[1 for i in range(anomalies.shape[0])]\n",
        "total_anomalies = pd.concat((total_anomalies,anomalyy),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6frieDYzBss"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbm68dngzBqH"
      },
      "source": [
        "loss_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnwIM-MvzBnq"
      },
      "source": [
        "anomaly_plot(test,anomalies=anomalies,col=\"zfreq\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOjMY7TB7K1"
      },
      "source": [
        "ariza_tarihleri_plot= []\n",
        "for i in [str(a) for a in data[\"Time\"]]:\n",
        "          if i not in [str(b) for b in ariza_tarihleri]:\n",
        "            ariza_tarihleri_plot.append(data[col].min())\n",
        "          else:\n",
        "            ariza_tarihleri_plot.append(data[col].max())\n",
        "ariza=pd.DataFrame(ariza_tarihleri_plot)\n",
        "ariza.index=data[\"Time\"]\n",
        "ariza.columns=[\"ariza tarihleri\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XV99BiY4qLG"
      },
      "source": [
        "total_anomalies.groupby(\"Time\")[\"anomaly_count\"].sum().plot()\n",
        "ariza[\"ariza tarihleri\"].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHk5vpqx4ysL"
      },
      "source": [
        "total_anomalies[\"Time\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7wnlDRTFAXB"
      },
      "source": [
        "total_anomalies.to_csv(\"total_anomaly.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44OcB13wdYa8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}